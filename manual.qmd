---
title: "Power analyses for typical statistical approaches"
subtitle: "A collaborative manual"
author:
  - name: "R Jour Fixe"
    affiliation: "Team SchuLe @ LLiB @ DIPF"
    correspondence: true
date: last-modified # inserts the date of last modification
editor: source
execute:
  warning: false  # don't show warnings (there are no problems if you ignore them)
  message: false  # don't show messages (e.g. when loading packages)
  cache: false # true will prevent embed-resources 
format: 
  html:
    theme: cerulean
    number-sections: false
    fontsize: 0.85em
    toc: true           # include table of contents
    toc-location: left
    toc-depth: 3        # table of contents up to third header level
    embed-resources: true # will make standalone html file
    code-fold: show    # show code and make it foldable
    code-tools: true   # show code tools on the top right
    code-link: true    # make functions clickable as links
editor_options: 
  chunk_output_type: console
css: www/style.css
---

![](www/power_meme.jpg)

# t-Test (Charlotte)

## Frequentistic (Charlotte)

## Bayesian (independent samples)

::: {.panel-tabset} 

#### What is tested?

> How high is the probability of true positive, inconclusive and false negative results at any given sample size?

**What we need to provide?**

- the range of sample sizes we want to investigate
- number of simulations to run (the more the ~merrier~ less variation)
- the smallest effect size of interest
- the hypothesis/hypotheses we want to compare

#### R script example

```{r}
library(bain)
library(tidyverse)
library(ggpubr)
```


**Specifications of power analysis**  

What range of sample size do you want to look at? You can choose your

- smallest sample size 
- biggest sample size
- the steps that R should go up from smallest to biggest

```{r}
# Which samples sizes are we looking at?
samplesize_min <- 40    # min sample size to begin with
samplesize_max <- 250   # max sample size to end with
samplesize_step <- 10   # step size between min and max
```
\

- How many simulations should R do for each sample size? The more simulations the more accurate the power analysis. I used 100 only for this script to run fast. Please use 1,000 to 10,000 for high accuracy.

```{r}
# how many simulations should do?
n_sim <- 100 # set Simulations to 1000 or higher!
```
\

- What is the smallest effect size of interest in your study?

```{r}
# what is the smallest effect size of interest?
smallest_es <- .36 # Taken from https://doi.org/10.1177%2F15562646211020160
                   # see Tab. 2 "low reading level" vs. "control"
                   # at DV "Comprehension"
```
\



**Power analysis**  
  
First set up an empty data frame to store the results in

```{r}
# make global data frame to store results in
# filling first column with all sample sizes
# results acheived will be added as columns
results_df <- data.frame(n = seq(samplesize_min, samplesize_max, samplesize_step))
```
\

Then we run the simulation 100 times for each sample size.

```{r}
## POWER ANALYSES ##############################################################
for (j in 1:n_sim) {  # loop over number of simulations
  # for this loop: make local data frame to store results in
  results_df_j <- data.frame(n = as.numeric(),   
                             BF = as.numeric())
  
  for (i in seq(samplesize_min, samplesize_max, samplesize_step)) { # loop over all sample sizes
    set.seed(123+i+j) # seed for reproducibility
    
    # simulate data set with two groups that differ in smallest_es 
    simdat <- data.frame(comprehension = c(rnorm(n = i/2, mean = 0, sd = 1), 
                                           rnorm(n = i/2, mean = smallest_es, sd = 1)),
                         reading_level = c(rep("standard", i/2), 
                                           rep("easy", i/2)))
    # compute t-test
    ttest <- t_test(comprehension ~ reading_level, data=simdat)
    
    set.seed(456+i+j)  # seed for reproducibility
    # Use bain to compute BFs for specific hypotheses
    results <- bain(ttest, "groupeasy = groupstandard; groupeasy > groupstandard")
    
    # store result as new row in local data frame
    results_df_j <- results_df_j %>%
      add_row(n = i,  # store sample size
              BF = results$fit$BF[2])  # store: we are interested in BF.c of H2
  }
  # rename column to indicate the current number of simulation
  names(results_df_j) <- c("n", paste0("BF_sim", j))
  # add column to global result data frame
  results_df <- left_join(results_df, results_df_j, by = "n")
}
```

#### Interpretation of results

**Wrangle and plot results**
```{r}
# wrangle results
results_df_l <- results_df %>%
  pivot_longer(contains("BF_sim"),  # make long data set
               names_to = "n_sim", 
               values_to = "BF") %>%
  mutate(BF_pos = ifelse(BF >= 5, 1, 0), # code BF larger than 5 as true positive
         BF_inc = ifelse(BF < 5 & BF > 1/5, 1, 0), # code BF between 5 and 1/5 as inconclusive
         BF_neg = ifelse(BF <= 1/5, 1, 0)) # code BF smaller than 1/5 as false negative


### TRUE POSITIVE
p1 <- ggplot(results_df_l, aes(x = n, y = BF_pos)) + 
  stat_summary(fun = mean) +                   # percent of simulations with true positive result
  geom_hline(yintercept = .8, color = "red") + # line for 80% power
  geom_hline(yintercept = .9, color = "darkred") + # line for 90% power
  scale_y_continuous(limits = c(0,1), expand = c(0,0)) +
  ggtitle("True positive") +
  ylab("share of true positive") +
  theme_light()

### INCONCLUSIVE
p2 <- ggplot(results_df_l, aes(x = n, y = BF_inc)) + 
  stat_summary(fun = mean) +
  scale_y_continuous(limits = c(0,1), expand = c(0,0)) +
  ggtitle("Inconclusive") +
  ylab("share of inconclusive") +
  theme_light()

### FALSE NEGATIVE
p3 <- ggplot(results_df_l, aes(x = n, y = BF_neg)) + 
  stat_summary(fun = mean) +                    # percent of simulations with false negative result
  geom_hline(yintercept = .2, color = "red") +  # line for beta = 20%
  scale_y_continuous(limits = c(0,1), expand = c(0,0)) +
  ggtitle("False negative") +
  ylab("share of false negative") +
  theme_light()

# Plot these togehter
ggarrange(p1, p2, p3,
          ncol = 2,
          nrow = 2)
```
\

* We see that from **N=130** the `true positive` (=Power) stays over .80 (=80%). So this would be a good option.
* We also see that from **N=160** the `inconclusive` results will drop and stay low. So if we don't want to run the risk of receiving inconclusive results, we might want to opt for this sample size.

\

**These are the exact numbers from the simulation**

```{r}
results_df_l
```

:::

## Paired samples

## Independent samples

# Regression

## Linear, multiple predictors (David)

## Logistic, multiple predictors

## Multilevel

## ANOVA, 2x2 between design (Anna)

::: {.panel-tabset} 

#### What is tested?

> How large does your sample have to be to make the expected effect visible?

We can also look for the achieved power, when providing a specific sample size.

**What we need to provide (when investigating sample size)?**

* Number of groups/conditions: 	individual for your study
    in R: k=
* Expected effect size (f): 	individual for your study
    in R: f=
* Alpha-level: 			.05
    in R: sig.level=.05
+ Power: 				0.8 or 0.9			
    in R: power=.9

#### Example 1: One-Way ANOVA

You want to know if students have a favorite subject among the subjects sports, art or music -> 3 groups. Derived from previous literature, you expect a moderate effect of 0.25. The level of significance should be 0.05 and the power = 0.8.

```{r}
library(pwr)

pwr.anova.test(k = 3, f = 0.25, sig.level = 0.05, power = 0.8)
```



**Participants needed (3 group * 52,3966): n = 157**

#### Example 2:Two Way ANOVA

You have two Factors, Factor A and Factor B, each has 2 categories/conditions. (2x2 = 4 group design; A1B1, A1B2, A2B1, A2B2). Derived from previous literature, for your overall model you expect a moderate effect of 0.25. The level of significance should be 0.05 and the power = 0.8.

This is the simple version...
...tests the overall model, instead of calculating the sample size for e.g. main effect A or B or their interaction isolated
... is the same as for 1-Factor-ANOVA (s.o.)

Only thing to mention:
k = number of groups in total, in our case you calculate 2x2->k = 4 

```{r}
library(pwr)

pwr.anova.test(k = 4,f= 0.25, sig.level = 0.05, power = 0.8)
```


**Participants needed (4 groups x 44,59): 178.36.**

:::

## ANOVA, repeated measures

## MANOVA

# CFA

# SEM



## Simple (Jürgen)



## Mediation Model

## Latent Growth Curve

## Multilevel?

## All SEM-models (analysis for fit)

::: {.panel-tabset} 

#### What is tested?

> How large does my sample have to be in order to acheive a pre-defined model fit?

Of course we can also look for the achieved power, when providing a specific sample size.

**What we need to provide (when investigating sample size)?**

- the alpha error
- the desired power
- the type of effect measure we want to work with
- the assumed magnitude of effect
- the degrees of freedom of the model


See <https://moshagen.github.io/semPower/#modelFreePower> for further details and explanations.

#### R script example

We are using a [SEM example](https://lavaan.ugent.be/tutorial/sem.html) from the lavaan tutorial website.  
  
The model:  
![](www/sem-model-1.png)  

We will be using the `semPower.aPriori()` function from the {semPower} package. 
This function needs the following arguments to be specified:

```{r}
#| eval: false

library(semPower)
req_sample <- semPower.aPriori(
                effect = ...,          # How large is the expected effect?
                effect.measure = ...,  # On which effect measure?
                alpha = ...,           # Which alpha level are we assuming?
                power = ...,           # Which power are we looking for?
                df = ...               # How many degrees of freedom does the model have?
                )
```
::: {.column-margin .margin-top-900}
*If you don't know how to determine the df of a model, check the [explanations from the semPower package](https://moshagen.github.io/semPower/#getDf) book.*
:::

\

Let's check how we want to **fill these arguments**:


1. **effect.measure**: If we investigate the overall fit of the model, we'll use the `effect.measure = "RMSEA"`. 
2. **effect**: Depending on what we expect, we can assume different goodness of fit in the RMSEA metric (Close Fit RMSEA < 0.05; Reasonable Fit: RMSEA between 0.05 and 0.08; Poor Fit: RMSEA > 0.08). Let's go with an okay-ish fit `effect = .08`
3. **alpha** and **power** are usually set to `alpha = .05` and `power = .8` based on conventions. Note: There might be good reasons to go for higher power.
4. **df**: Lastly, we need to **determine the degrees of freedom** of our model. We can either let the function `semPower.getDf()` do this for us or we can embark on the adventurous quest of determining it by hand.

::: {.panel-tabset} 

##### `semPower.getDf()`

To get the df of your model, simply define it in a `lavaan` style syntax (see [here on how to do that](https://lavaan.ugent.be/tutorial/syntax1.html)). And then run the function `semPower.getDf()` on the object.

```{r}
library(semPower)
model <- '
  # measurement model
    ind60 =~ x1 + x2 + x3
    dem60 =~ y1 + y2 + y3 + y4
    dem65 =~ y5 + y6 + y7 + y8
  # regressions
    dem60 ~ ind60
    dem65 ~ ind60 + dem60
  # residual correlations
    y1 ~~ y5
    y2 ~~ y4 + y6
    y3 ~~ y7
    y4 ~~ y8
    y6 ~~ y8
'

semPower.getDf(model)
```

The result (as seen above) is `r semPower.getDf(model)`.

##### Determine by hand
Wow. You are brave. Ok let's do this.

To determine the df of a SEM model we need the *number of observed variables* and the *number of parameters to be estimated*. In this case we have 11 observed variables (in rectangular boxes) and the parameters estimated are:

- 4 loadings from `dem60` to `y1`, `y2`, `y3`, `y4`
- 4 loadings from `dem65` to `y5`, `y6`, `y7`, `y8`
- 3 loadings from `ind60` to `x1`, `x2`, `x3`
- 11 error variances of all observed variables (from `y1` to `x3`)
- 1 latent prediction path from `dem60` to `dem65`
- 1 latent prediction path from `ind60` to `dem60`
- 1 latent prediction path from `ind60` to `dem65`
- 6 error correlations (e.g., `y1` with `y5`)

Applying [this guy's math](https://www.youtube.com/watch?v=yt4rjnAW2QM) in the formula $df = p\times(p+1)/2−q$ (where $p$ number of observed variables and $q$ is the number of free parameters of the hypothesized model):

$$df = 11\times(11+1)/2−31 = 35$$
We'll have **35** *df*.
:::




\

So if we **plug these in**, our function looks like this:

```{r}
library(semPower)
req_sample <- semPower.aPriori(effect = .08,
                               effect.measure = 'RMSEA',
                               alpha = .05, 
                               power = .80, 
                               df = 35)
```

#### Interpretation of results
```{r}
summary(req_sample)
```

\

In the row `Required Num Observations` indicates the $N$ needed to detect a fit of RMSEA = .08 with a probability of 80% (power): **`r req_sample$requiredN`**.

::: {.callout-note collapse="true" title="Explanation of the curves"}
The plot shows two chi-square distributions: the central chi-square distribution under the null hypothesis (in red) and the non-central chi-square distribution under the alternative hypothesis (in blue).

**Null Hypothesis ($H_0$)**

The null hypothesis states that the model fits the data with an RMSEA value equal to or greater than the specified threshold value. In this case, the threshold value is 0.08, which is often considered the cutoff for acceptable model fit.  

Formally: $H_0: RMSEA \geq 0.08$

**Alternative Hypothesis ($H_1$)**

The alternative hypothesis states that the model fits the data with an RMSEA value less than the specified threshold value.  

Formally: $H_1: RMSEA \leq 0.08$

**Lines in the plot**

- Central Chi-Square Distribution (red solid line): Represents the distribution of the chi-square statistic if the null hypothesis is true. The critical chi-square value (approximately 48.60) marks the threshold above which we would reject the null hypothesis.
- Non-Central Chi-Square Distribution (blue dashed line): Represents the distribution of the chi-square statistic if the alternative hypothesis is true. The peak of this distribution is shifted to the right, indicating that the true model deviates from the null model.

**Error Regions**

- Alpha Error (Type I Error): The area under the central chi-square curve to the right of the critical chi-square value. This area represents the probability of incorrectly rejecting the null hypothesis when it is true (false positive).
- Beta Error (Type II Error): The area under the non-central chi-square curve to the left of the critical chi-square value. This area represents the probability of incorrectly failing to reject the null hypothesis when it is false (false negative).

**Power**

- The power of the test (1 - Beta) is represented by the area under the non-central chi-square curve to the right of the critical chi-square value. This area indicates the probability of correctly rejecting the null hypothesis when the alternative hypothesis is true.
:::

:::