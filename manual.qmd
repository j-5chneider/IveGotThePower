---
title: "Power analyses for typical statistical approaches"
subtitle: "A collaborative manual"
author:
  - name: "R Jour Fixe"
    affiliation: "Team SchuLe @ LLiB @ DIPF"
    correspondence: true
date: last-modified # inserts the date of last modification
editor: source
execute:
  warning: false  # don't show warnings (there are no problems if you ignore them)
  message: false  # don't show messages (e.g. when loading packages)
  cache: false # true will prevent embed-resources 
format: 
  html:
    theme: cerulean
    number-sections: false
    fontsize: 0.85em
    toc: true           # include table of contents
    toc-location: left
    toc-depth: 3        # table of contents up to third header level
    embed-resources: true # will make standalone html file
    code-fold: show    # show code and make it foldable
    code-tools: true   # show code tools on the top right
    code-link: true    # make functions clickable as links
editor_options: 
  chunk_output_type: console
css: www/style.css
---

![](www/power_meme.jpg)

# t-Test (Charlotte)

## Frequentistic (Charlotte)

## Bayesian (independent samples)

::: {.panel-tabset} 

#### What is tested?

> How high is the probability of true positive, inconclusive and false negative results at any given sample size?

**What we need to provide?**

- the range of sample sizes we want to investigate
- number of simulations to run (the more the ~merrier~ less variation)
- the smallest effect size of interest
- the hypothesis/hypotheses we want to compare

#### R script example

```{r}
library(bain)
library(tidyverse)
library(ggpubr)
```


**Specifications of power analysis**  

What range of sample size do you want to look at? You can choose your

- smallest sample size 
- biggest sample size
- the steps that R should go up from smallest to biggest

```{r}
# Which samples sizes are we looking at?
samplesize_min <- 40    # min sample size to begin with
samplesize_max <- 250   # max sample size to end with
samplesize_step <- 10   # step size between min and max
```
\

- How many simulations should R do for each sample size? The more simulations the more accurate the power analysis. I used 100 only for this script to run fast. Please use 1,000 to 10,000 for high accuracy.

```{r}
# how many simulations should do?
n_sim <- 100 # set Simulations to 1000 or higher!
```
\

- What is the smallest effect size of interest in your study?

```{r}
# what is the smallest effect size of interest?
smallest_es <- .36 # Taken from https://doi.org/10.1177%2F15562646211020160
                   # see Tab. 2 "low reading level" vs. "control"
                   # at DV "Comprehension"
```
\



**Power analysis**  
  
First set up an empty data frame to store the results in

```{r}
# make global data frame to store results in
# filling first column with all sample sizes
# results acheived will be added as columns
results_df <- data.frame(n = seq(samplesize_min, samplesize_max, samplesize_step))
```
\

Then we run the simulation 100 times for each sample size.

```{r}
## POWER ANALYSES ##############################################################
for (j in 1:n_sim) {  # loop over number of simulations
  # for this loop: make local data frame to store results in
  results_df_j <- data.frame(n = as.numeric(),   
                             BF = as.numeric())
  
  for (i in seq(samplesize_min, samplesize_max, samplesize_step)) { # loop over all sample sizes
    set.seed(123+i+j) # seed for reproducibility
    
    # simulate data set with two groups that differ in smallest_es 
    simdat <- data.frame(comprehension = c(rnorm(n = i/2, mean = 0, sd = 1), 
                                           rnorm(n = i/2, mean = smallest_es, sd = 1)),
                         reading_level = c(rep("standard", i/2), 
                                           rep("easy", i/2)))
    # compute t-test
    ttest <- t_test(comprehension ~ reading_level, data=simdat)
    
    set.seed(456+i+j)  # seed for reproducibility
    # Use bain to compute BFs for specific hypotheses
    results <- bain(ttest, "groupeasy = groupstandard; groupeasy > groupstandard")
    
    # store result as new row in local data frame
    results_df_j <- results_df_j %>%
      add_row(n = i,  # store sample size
              BF = results$fit$BF[2])  # store: we are interested in BF.c of H2
  }
  # rename column to indicate the current number of simulation
  names(results_df_j) <- c("n", paste0("BF_sim", j))
  # add column to global result data frame
  results_df <- left_join(results_df, results_df_j, by = "n")
}
```

#### Interpretation of results

**Wrangle and plot results**
```{r}
# wrangle results
results_df_l <- results_df %>%
  pivot_longer(contains("BF_sim"),  # make long data set
               names_to = "n_sim", 
               values_to = "BF") %>%
  mutate(BF_pos = ifelse(BF >= 5, 1, 0), # code BF larger than 5 as true positive
         BF_inc = ifelse(BF < 5 & BF > 1/5, 1, 0), # code BF between 5 and 1/5 as inconclusive
         BF_neg = ifelse(BF <= 1/5, 1, 0)) # code BF smaller than 1/5 as false negative


### TRUE POSITIVE
p1 <- ggplot(results_df_l, aes(x = n, y = BF_pos)) + 
  stat_summary(fun = mean) +                   # percent of simulations with true positive result
  geom_hline(yintercept = .8, color = "red") + # line for 80% power
  geom_hline(yintercept = .9, color = "darkred") + # line for 90% power
  scale_y_continuous(limits = c(0,1), expand = c(0,0)) +
  ggtitle("True positive") +
  ylab("share of true positive") +
  theme_light()

### INCONCLUSIVE
p2 <- ggplot(results_df_l, aes(x = n, y = BF_inc)) + 
  stat_summary(fun = mean) +
  scale_y_continuous(limits = c(0,1), expand = c(0,0)) +
  ggtitle("Inconclusive") +
  ylab("share of inconclusive") +
  theme_light()

### FALSE NEGATIVE
p3 <- ggplot(results_df_l, aes(x = n, y = BF_neg)) + 
  stat_summary(fun = mean) +                    # percent of simulations with false negative result
  geom_hline(yintercept = .2, color = "red") +  # line for beta = 20%
  scale_y_continuous(limits = c(0,1), expand = c(0,0)) +
  ggtitle("False negative") +
  ylab("share of false negative") +
  theme_light()

# Plot these togehter
ggarrange(p1, p2, p3,
          ncol = 2,
          nrow = 2)
```
\

* We see that from **N=130** the `true positive` (=Power) stays over .80 (=80%). So this would be a good option.
* We also see that from **N=160** the `inconclusive` results will drop and stay low. So if we don't want to run the risk of receiving inconclusive results, we might want to opt for this sample size.

\

**These are the exact numbers from the simulation**

```{r}
results_df_l
```

:::

## Paired samples

## Independent samples

# Regression

## Linear, multiple predictors (David)

::: {.panel-tabset} 

#### What is tested?

**When running a multiple regression, there are two ways to think about the power you may or may not have.**

-  Focus on having a sufficient power to the variance accounted for by the set of predictors (i.e. R2) 
-  Focus on having a sufficient power to the unique variance accounted for by a single predictor

Package used for this sequence:
[https://CRAN.R-project.org/package=pwrss](https://CRAN.R-project.org/package=pwrss )

#### I've got R2: What is the minimum sample size?

We want to run a multiple regression study in an existing research area using 3 predictors. A past study found an R2 of .30. 

We are expecting that the variables in our linear regression will explain 30% of the variance in the outcome (`r2 = 0.30`) as well. How many people do we need in our study?

**What we need to provide (when investigating sample size, given R2)?**

- Number of predictor terms (is degrees of freedom): 	`k = 3` (factual)
- Alpha-level: `alpha = 0.05` (convention)
- Desired power: `power = 0.80` (convention)
- and of course the desired R2: `r2 = 0.30 ` (i.e. previous research, or other thoughts)

```{r}
# install.packages("pwrss")
library(pwrss)

pwrss.f.reg(r2 = 0.30, k = 3, alpha = 0.05, power = 0.80)
```

[linked source](https://cran.r-project.org/web/packages/pwrss/vignettes/examples.html#3_Linear_Regression_(F_and_t_Tests))

**Participants needed: n = 30**

#### I've got R2 change: What is the minimum sample size?

We are looking at the same study, that used 3 predictor and found an R2 = .30. 

We expect predictor C will account for 2% of the variance in the criterion above and beyond the other two predictors. How many people do we need in our study?

**What we need to provide (when investigating sample size, given R-squared change)?**

Same as above:

- Number of predictor terms (is degrees of freedom): 	`k = 3` 
- Alpha-level: `alpha = 0.05` 
- Desired power: `power = 0.80`

And (new):

- number predictor terms whose incremental contribution to R-squared change is of interest, here: `m = 1` 
- f2: Expected Cohen's f-squared (an alternative to R2 specification), here: `f2 = r2 / (1 - r2) = 0.02 / (1 - 0.30) = 0.029`

```{r}
library(pwrss)

pwrss.f.reg(f2 = 0.029, k = 3, m = 1, alpha = 0.05, , power = 0.80)
```

[linked source](https://dstanley4.github.io/psyc4780bookdown/sample-size-for-multiple-regression.html#sample-size-using-sr2) 

**Participants needed: n = 273**

#### What else?

> The pwrss-package gives you whatever you need:

If you have done your study and want to know your power, this is possible.
Let's look at the first example above:

```{r}
library(pwrss)

pwrss.f.reg(r2 = 0.30, k = 3, n = 30, alpha = 0.05)
```

**Statistical power = 0.806; lucky you**

#### Talk beta to me!

We again want to predict a variable y using three predictors x1, x2 and x3. We are mainly interested in the effect of X1 and expect a standardized regression coefficient of β1 = 0.20. What is the minimum required sample size? 

```{r}
# library(pwrss)

pwrss.t.reg(beta1 = 0.20, k = 3, power = .80, alpha = 0.05)
```

**Participants needed: n = 191**

You can also add R squared again, if there is information available:

```{r}
pwrss.t.reg(beta1 = 0.20, k = 3, power = .80, alpha = 0.05, r2 = 0.30)
```

**Participants needed: n = 140**

[linked source](https://cran.r-project.org/web/packages/pwrss/vignettes/examples.html#32_Single_Regression_Coefficient_(z_or_t_Test)) 

#### A way to specify every piece of the model: Simulated power

"One can just use a simulated power analysis to specify every piece of the model that they are weighting if they are interested in a single coefficient. For example, if we want to specify the exact beta coefficients of each predictor, their standard error, the sample size, or any other metric, we could just simulate it in R directly." [found here](https://stats.stackexchange.com/a/643622) 

**What we can provide when doing a simulated power analysis?**

A regression model looks like `y = (beta0) + (beta1 * x1) + (beta2 * x2) + e`

So we could specify every part

- Sample size `n = `
- Intercept: `beta0 = `
- The slope of x1: `beta1 = `
- The slope of x2: `beta2 = `
- The residual standard error of the model: `e = `

In the following we will try to match the example of the section "Talk beta to me!", to see if this is right,

We looked at 3 predictors, were interested in the effect of x1 and expected a standardized regression coefficient of β1 = 0.20. The residual standard error of the model was not specified.

`y = (beta0) + (0.20 * x1) + (x2) + (x3)`

First, we will generate data of this model:

```{r}
# Data Generator Function

gen.data <- function(n, beta1){
  x1 <- rnorm(n)
  x2 <- rnorm(n)
  x3 <- rnorm(n)
  y <- (beta1 * x1) + x2 +x3
  df <- data.frame(x1, x2, x3, y)
  return(df)
}
```

Second, we set up the simulation:

```{r}
# Setup Simulation

boot <- 1000
alpha <- 0.05  # significance level
p <- numeric(boot)
n <- 191  # sample size (taken from the previous example)
beta1 <- .20 # slope 1
```

Third, we run the simulation:

```{r}
# Setup simulation

for(i in 1:boot){
  df <- gen.data(
    n = n,
    beta1 = beta1
    )
  fit <- lm(y ~ x1 + x2 + x3, df)
  p_values <- summary(fit)$coefficients[,4]
  p[i] <- p_values[2]
}
```

Last, we can calculate the power:

```{r}
# Calculate power
power <- mean(p < alpha)

# check power
power
```

**Statistical power = 0.8 ** -.-

:::

## Logistic, multiple predictors

## Multilevel

## ANOVA, 2x2 between design (Anna)

::: {.panel-tabset} 

#### What is tested?

> How large does your sample have to be to make the expected effect visible?

We can also look for the achieved power, when providing a specific sample size.

**What we need to provide (when investigating sample size)?**

* Number of groups/conditions: 	individual for your study
    in R: k=
* Expected effect size (f): 	individual for your study
    in R: f=
* Alpha-level: 			.05
    in R: sig.level=.05
+ Power: 				0.8 or 0.9			
    in R: power=.9

#### Example 1: One-Way ANOVA

You want to know if students have a favorite subject among the subjects sports, art or music -> 3 groups. Derived from previous literature, you expect a moderate effect of 0.25. The level of significance should be 0.05 and the power = 0.8.

```{r}
library(pwr)

pwr.anova.test(k = 3, f = 0.25, sig.level = 0.05, power = 0.8)
```



**Participants needed (3 group * 52,3966): n = 157**

#### Example 2: Two Way ANOVA

You have two Factors, Factor A and Factor B, each has 2 categories/conditions. (2x2 = 4 group design; A1B1, A1B2, A2B1, A2B2). Derived from previous literature, for your overall model you expect a moderate effect of 0.25. The level of significance should be 0.05 and the power = 0.8.

This is the simple version...
...tests the overall model, instead of calculating the sample size for e.g. main effect A or B or their interaction isolated
... is the same as for 1-Factor-ANOVA (s.o.)

Only thing to mention:
k = number of groups in total, in our case you calculate 2x2->k = 4 

```{r}
library(pwr)

pwr.anova.test(k = 4,f= 0.25, sig.level = 0.05, power = 0.8)
```


**Participants needed (4 groups x 44,59): 178.36.**


#### Example 3: Two Way Between ANOVA

Let's imagine we measure reaction times to a red or blue dot being displayed on a screen. We set out to test old vs. young people. Our assumption is, that old people recognize the blue dot quicker (lower reaction time) and young people recognize the red dot quicker (lower reaction time).  
  
This results in a 2x2 between design with the effect of interest being the interaction effect.


```{r}
library(Superpower)

mu <- c(-0.25, 0.25, 0.25,-0.25) # Which mean differences do we expect?
n <- 23             # How large is the sample?
sd <- 1             # standard deviation, leave at 1 so that mean differences (mu) can be interpreted as ~ Cohen's d
string = "2b*2b"    # 2 between factors with 2 groups each
alpha_level <- 0.05 # as is tradition
labelnames = c("age", "old",      # Label OF groups (optional)
               "young", "color", 
               "blue", "red")

# Definition of design, so that R can simulate the data
design_result <- ANOVA_design(design = string,
                              n = n,
                              mu = mu,
                              sd = sd,
                              labelnames = labelnames)

# number of simulations
# this should usually be a lot more (1,000 - 10,000)
# but with 100 the script runs faster
nsims <- 100

set.seed(1100101) # this will enable reproducible results

# run simulation
simulation_result <- ANOVA_power(design_result, 
                                 alpha_level = alpha_level, 
                                 nsims = nsims,
                                 verbose = TRUE)
```
\

We see that the power of our interaction effect (`anova_age:color`) with `N=23` per group is **78%**. That's not quite enough. We therefore need to rise the N a bit more.



#### Example 4: Two Way Within-Between ANOVA

Let's imagine we measure reaction times to a red or blue dot being displayed on a screen. We set out to test young people and test **the same** people a couple years later (="old"). Our assumption is, that when old, people recognize the blue dot quicker (lower reaction time) than when they were young. When they were young, however, they recognized the red dot quicker (lower reaction time).  
  
This results in a 2x2 within-between design with the effect of interest being the interaction effect.

```{r}
library(Superpower)

mu <- c(-0.25, 0.25, 0.25,-0.25) # Which mean differences do we expect?
n <- 23             # How large is the sample?
sd <- 1             # standard deviation, leave at 1 so that mean 
                    # differences (mu) can be interpreted as ~ Cohen's d

string = "2w*2b"    # 2 between factors with 2 groups each
r <- 0.5            # which correlation of the within measurement (e.g. pre-post) do we expect?
alpha_level <- 0.05 # as is tradition
labelnames = c("age", "old", "young", # Label OF groups (optional)
               "color", "blue", "red")

# Definition of design, so that R can simulate the data
design_result <- ANOVA_design(design = string,
                              n = n,
                              mu = mu,
                              sd = sd,
                              labelnames = labelnames)

# number of simulations
# this should usually be a lot more (1,000 - 10,000)
# but with 100 the script runs faster
nsims <- 100

set.seed(1100101) # this will enable reproducible results

# run simulation
simulation_result <- ANOVA_power(design_result, 
                                 alpha_level = alpha_level, 
                                 nsims = nsims,
                                 verbose = TRUE)
```

We see that the power of our interaction effect (`anova_age:color`) with `N=23` per group is **74%**. That's not quite enough. We therefore need to rise the N a bit more.

:::

## ANOVA, repeated measures

## MANOVA

# CFA

# SEM



## Simple (Jürgen)

<https://lmu-osc.github.io/Simulations-for-Advanced-Power-Analyses/SEM.html#using-the-lavaan-syntax-to-simulate-data>

## Mediation Model

## Latent Growth Curve

## Multilevel?

## All SEM-models (analysis for fit)

::: {.panel-tabset} 

#### What is tested?

> How large does my sample have to be in order to acheive a pre-defined model fit?

Of course we can also look for the achieved power, when providing a specific sample size.

**What we need to provide (when investigating sample size)?**

- the alpha error
- the desired power
- the type of effect measure we want to work with
- the assumed magnitude of effect
- the degrees of freedom of the model


See <https://moshagen.github.io/semPower/#modelFreePower> for further details and explanations.

#### R script example

We are using a [SEM example](https://lavaan.ugent.be/tutorial/sem.html) from the lavaan tutorial website.  
  
The model:  
![](www/sem-model-1.png)  

We will be using the `semPower.aPriori()` function from the {semPower} package. 
This function needs the following arguments to be specified:

```{r}
#| eval: false

library(semPower)
req_sample <- semPower.aPriori(
                effect = ...,          # How large is the expected effect?
                effect.measure = ...,  # On which effect measure?
                alpha = ...,           # Which alpha level are we assuming?
                power = ...,           # Which power are we looking for?
                df = ...               # How many degrees of freedom does the model have?
                )
```
::: {.column-margin .margin-top-900}
*If you don't know how to determine the df of a model, check the [explanations from the semPower package](https://moshagen.github.io/semPower/#getDf) book.*
:::

\

Let's check how we want to **fill these arguments**:


1. **effect.measure**: If we investigate the overall fit of the model, we'll use the `effect.measure = "RMSEA"`. 
2. **effect**: Depending on what we expect, we can assume different goodness of fit in the RMSEA metric (Close Fit RMSEA < 0.05; Reasonable Fit: RMSEA between 0.05 and 0.08; Poor Fit: RMSEA > 0.08). Let's go with an okay-ish fit `effect = .08`
3. **alpha** and **power** are usually set to `alpha = .05` and `power = .8` based on conventions. Note: There might be good reasons to go for higher power.
4. **df**: Lastly, we need to **determine the degrees of freedom** of our model. We can either let the function `semPower.getDf()` do this for us or we can embark on the adventurous quest of determining it by hand.

::: {.panel-tabset} 

##### `semPower.getDf()`

To get the df of your model, simply define it in a `lavaan` style syntax (see [here on how to do that](https://lavaan.ugent.be/tutorial/syntax1.html)). And then run the function `semPower.getDf()` on the object.

```{r}
library(semPower)
model <- '
  # measurement model
    ind60 =~ x1 + x2 + x3
    dem60 =~ y1 + y2 + y3 + y4
    dem65 =~ y5 + y6 + y7 + y8
  # regressions
    dem60 ~ ind60
    dem65 ~ ind60 + dem60
  # residual correlations
    y1 ~~ y5
    y2 ~~ y4 + y6
    y3 ~~ y7
    y4 ~~ y8
    y6 ~~ y8
'

semPower.getDf(model)
```

The result (as seen above) is `r semPower.getDf(model)`.

##### Determine by hand
Wow. You are brave. Ok let's do this.

To determine the df of a SEM model we need the *number of observed variables* and the *number of parameters to be estimated*. In this case we have 11 observed variables (in rectangular boxes) and the parameters estimated are:

- 4 loadings from `dem60` to `y1`, `y2`, `y3`, `y4`
- 4 loadings from `dem65` to `y5`, `y6`, `y7`, `y8`
- 3 loadings from `ind60` to `x1`, `x2`, `x3`
- 11 error variances of all observed variables (from `y1` to `x3`)
- 1 latent prediction path from `dem60` to `dem65`
- 1 latent prediction path from `ind60` to `dem60`
- 1 latent prediction path from `ind60` to `dem65`
- 6 error correlations (e.g., `y1` with `y5`)

Applying [this guy's math](https://www.youtube.com/watch?v=yt4rjnAW2QM) in the formula $df = p\times(p+1)/2−q$ (where $p$ number of observed variables and $q$ is the number of free parameters of the hypothesized model):

$$df = 11\times(11+1)/2−31 = 35$$
We'll have **35** *df*.
:::




\

So if we **plug these in**, our function looks like this:

```{r}
library(semPower)
req_sample <- semPower.aPriori(effect = .08,
                               effect.measure = 'RMSEA',
                               alpha = .05, 
                               power = .80, 
                               df = 35)
```

#### Interpretation of results
```{r}
summary(req_sample)
```

\

In the row `Required Num Observations` indicates the $N$ needed to detect a fit of RMSEA = .08 with a probability of 80% (power): **`r req_sample$requiredN`**.

::: {.callout-note collapse="true" title="Explanation of the curves"}
The plot shows two chi-square distributions: the central chi-square distribution under the null hypothesis (in red) and the non-central chi-square distribution under the alternative hypothesis (in blue).

**Null Hypothesis ($H_0$)**

The null hypothesis states that the model fits the data with an RMSEA value equal to or greater than the specified threshold value. In this case, the threshold value is 0.08, which is often considered the cutoff for acceptable model fit.  

Formally: $H_0: RMSEA \geq 0.08$

**Alternative Hypothesis ($H_1$)**

The alternative hypothesis states that the model fits the data with an RMSEA value less than the specified threshold value.  

Formally: $H_1: RMSEA \leq 0.08$

**Lines in the plot**

- Central Chi-Square Distribution (red solid line): Represents the distribution of the chi-square statistic if the null hypothesis is true. The critical chi-square value (approximately 48.60) marks the threshold above which we would reject the null hypothesis.
- Non-Central Chi-Square Distribution (blue dashed line): Represents the distribution of the chi-square statistic if the alternative hypothesis is true. The peak of this distribution is shifted to the right, indicating that the true model deviates from the null model.

**Error Regions**

- Alpha Error (Type I Error): The area under the central chi-square curve to the right of the critical chi-square value. This area represents the probability of incorrectly rejecting the null hypothesis when it is true (false positive).
- Beta Error (Type II Error): The area under the non-central chi-square curve to the left of the critical chi-square value. This area represents the probability of incorrectly failing to reject the null hypothesis when it is false (false negative).

**Power**

- The power of the test (1 - Beta) is represented by the area under the non-central chi-square curve to the right of the critical chi-square value. This area indicates the probability of correctly rejecting the null hypothesis when the alternative hypothesis is true.
:::

:::